\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
    Despite achieving state-of-the-art performance in lab-conditions,Â deep learning-based systems often exhibit significant performance degradation when deployed in practical settings. This is referred to as \textit{generalization failure}. Why and how this occurs has only recently started to be understood, and there has consequently been limited research towards developing generalizable methods for deep learning. 
    
    This thesis attempts to address generalization failure in the domain of medical image segmentation, in particular in the context of  segmentation of colorectal polyps. Recent analyses of generalizability is discussed, which is then used to inform the development of a number of novel methods. This includes a simple dual-decoder architecture, an augmentation strategy which incorporates a deep inpainting model that adds polyps to a given image, a novel training paradigm referred to as \textit{Consistency Training}, and finally, several ensemble models for which the constituent predictors are trained using Consistency Training.
    
    These methods are then evaluated quantitatively through several comparative studies. As the extent to which the methods used as baselines in this thesis affect generalization is not particularly well understood, this thesis also contributes a quantitative analysis of the the impact of the choice of model architecture, the use of data augmentation, and the use of ensemble-models on generalization. 
    
    Our results show that Consistency Training outperforms all other tested methods. Data augmentation induces similar, but slightly lower levels of generalization. The use of the deep inpainter model as a component of data augmentation, however, limits the possible improvements compared to regular augmentation. Ensembles improve generalization, albeit to a somewhat lesser extent than the aforementioned methods. Finally, the choice of model architecture, including the use of a secondary decoder, was shown to have negligible positive effects on generalization. 
    
    These findings are then analyzed and used to inform a number of hypotheses which are suggested at points of further study. Several improvements for the proposed methods are also suggested, in particular with regards to Consistency Training, which shows significant promise towards further mitigating generalization failure. 