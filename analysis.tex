\chapter{Analysis}\label{analysis}
Key findings:
\begin{itemize}
    \item The models exhibit higher variance in OOD generalization than IID generalization
    \item Larger nets do not necessarily mean better generalization
    \item Ensembles help, and diverse ensembles (...)
    \item SIL performs similarly to data augmentation, both of which considerably increase generaliability
    \item SIS is a decent indicator of generalizability, but evaluating on augmented sets likely yields similar results. Consistency is, however, more intitive
    \item Inpainting harms generalization. This may be because it strengthens shortcuts or that the polyps simply are not realistic enough. Further experimentation may be useful to map out precisely the effects of ind generative modelling 
\end{itemize}

\section{Model impact on generalization}
\section{GANs and Generalizability Failure}
\section{SIS vs InD validation}
\section{SIL training}
\subsection{Augmentation Robustness and Consistency Loss}
As the results show, the performance of the pipeline that merely used augmentations is more or less equivalent to the performance exhibited by the modified pipeline. There is a very good reason for this: Consistency loss is mathematically equivalent to data augmentation, up to the choice of hyperparemeters - i.e augmentation probability, learning rates, etc. This section presents a proof of this fact, along with a theoretical analysis of how data augmentation affects the pipeline. 
\subsubsection{Data augmentation}
Let \(Y:=\{y,\hat{y}:=f(x)\}\) be the set consisting of the segmentation predictions and masks for the unaugmented samples, and \(A:=\{a:=MNV(y),\hat{a}:=f(MNV(x))\}\) be the set consisting of segmentation predictions and masks for the augmented samples. Finally, let \(Z:=\{z, \hat{z}\} \in_R \{Y, A\} \). The loss function subject to data augmentation can then be expressed as \(L(Z \in_R Y,A)\), where L is any loss function. For the sake of simplicity in remaining calculations, this will be treated as the Jaccard loss, i.e \(L(y,\hat{y}):=1-\sum y\cap \hat{y} / \sum y \cup \hat{y}\) 
\begin{align*}
L(Z \in_R Y,A)  
\end{align*}
\subsubsection{Consistency loss}

\input{loss_analysis_proof}
The non-loss terms in equation \ref{loss_proof} are proper subsets of the symmetric difference of the mask and segmentation across either dataset. The component of the loss that corresponds to these terms consequently grows in proportion to both \(L_s(y, \hat{y})\) and \(L_s(a, \hat{a})\). \(L_{c+s}\) and \(L_{sy+sa}\) are therefore monotonically correlated - i.e, when one grows, the other grows with it, and when one falls, the other one falls with it. 
\section{Ensembles and Generalization}
