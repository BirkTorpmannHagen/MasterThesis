\chapter{Analysis and Discussion}\label{discussion}
This chapter will summarize the key findings presented in~\Cref{experiments} and analyze them with respect to the theory as outlined in~\Cref{background}. The chapter will be organized according to the experiments performed, with each section discussing the results, impact, and limitations of the corresponding experiment. The chapter will start with the results from the individual experiments, including the impact of model architectures on generalization as presented in~\Cref{models}, the impact of augmentation as presented in~\Cref{augmentations}, inconsistency Training as presented in~\Cref{consistency_training}, and finally ensembles as presented in~\Cref{ensembles}. Afterwards, the generalizability of the best performing configuration tested in this thesis will be discussed and considered from a practical perspective.


\section{Model Architectures and Generalizability}\label{discussion:models}
The experiments performed in~\Cref{models} show that every model exhibited comparable levels of generalisation failure, with the exception of TriUnet which seemed to struggle more than any of the other models. On Etis-LaribDB, which evidently proved to be the most difficult dataset, every model exhibited a generalizability gap of at least around 50\%, with the TriUnet ranging upwards of 65\%. The degree of generalization failure was slightly less pronounced on the two other datasets, with CVC-ClinicDB exhibiting average gaps of approximately 18\% and EndoCV2020 25\%. 

The models exhibited comparable performance also in \gls{ind} settings, spanning between IoUs of 0.819 and 0.832, which for practical purposes can be considered negligible. 

\subsection{Impact}
Naturally, deploying any of the predictors from this experiment in a clinical setting would be inadvisable and perhaps even harmful. Even if the predictors were to be trained on a dataset collected exclusively from the centre they were intended to be deployed on, there is no guarantee that there would never be some form of distributional shift, which as evidenced by the results significantly affect performance. This distributional shift may be as simple as a change in endoscopy preparation routines, or perhaps an upgrade to a higher resolution cameras, and so on. As established in~\Cref{background}, a system based on models with this lack of generalizability would be practically useless. 

Moreover, these results highlight that researching the development of more and more complicated task-agnostic models is a comparatively fruitless affair. The difference between DeepLabV3+ and Unet - which are separated by two years of research - are practically inconsequential. Admittedly, the differences are more pronounced when the models are trained according to the more sophisticated training regiments used in the remaining experiments, but it is nonetheless clear that it is not advancement in model architectures that is likely to result in increased generalization, but rather improvements to the pipeline with which they are trained.

\subsection{Limitations}\label{pretraining}
        A largely neglected but nevertheless impactful aspect of the deep learning pipelines studied in this thesis is the use of pretraining. Across all the experiments performed in this thesis, every predictor was pretrained on Imagenet, with the pretrained weights being included in the segmentation-models-pytorch library~\cite{smp}. Without pretraining, the models selected in this thesis exhibited \glspl{iou} of at best around 0.6 at best even on \gls{iid}, with even more significant performance gaps on \gls{ood} data. Naturally, non-pretrained networks are for this reason rarely used. However, this pretraining may play a key role in certain aspects of the behaviour observed in this thesis. In particular, pretraining may be the principle contributing factor behind the apparent ineffectiveness of multitask learning. An Imagenet pretrained encoder would, after all, perform the best when practically performing image compression.

\section{Data Augmentation and Generalizability}
The experiment in~\Cref{augmentations} demonstrated the efficacy of data augmentation as a means of increasing generalization, with an \gls{iou} improvement of 9.00\% compared to the pipeline without data augmentation when averaged across models and datasets. This, as mentioned in~\Cref{probpersp}, can be attributed to the fact that a wider diversity of data limits the space of viable features that a model can learn. 

\subsection{Impact}

What is surprising is the extent to which augmentation improves generalization in comparison to some of the other tested methods. In particular, the effects of model architectures and ensembles were both comparatively small. The use of ensembles, for instance, the use of which was the basis of several of the papers submitted to EndoCV2021, increased generalization by at most \(10.36\%\) and on average \(2.48\%\), whereas the use of data augmentation led to increases of at most \(19.57 \%\) and on average \(9.00\%\) when compared to no augmentation.

Thus, the margins by which the use of augmentation affects generalization are far greater than the margins by which ensembles and perhaps many of the other methods presented in EndoCV2021 affect generalization. This raises questions as to the validity of the results in EndoCV2021, which did not control for the choice of data augmentations when comparing submissions. It may, for instance, be the case that certain submissions exhibited high degrees of generalization not strictly because of the impact of their proposed methods, but rather due to their choice of augmentations. This is of course not a certainty, and does as such warrant further research for instance in the form of a meta-analysis. 

\subsection{Limitations} \label{augmentation_limitations}
    Ignoring the inpainter and its flaws as outlined above, only one implementation of data augmentation was used throughout this thesis. The constituent transformations and the values of the hyperparameters thereof were also selected with limited prototyping or testing. There may as such be augmentation configurations that induce significantly increased generalization. By the same token, the selection of transformations used in this thesis may instead have been lucky and thus over-represent the typical contribution of data augmentation. A robust investigation of data augmentation and its effects would require a larger range of augmentation strategies. The results thereof would, however, only be of relevance to the particular task that is being considered. Polyp segmentation may benefit more from augmentation than image-captioning, for instance. 
    
    Additionally, the augmentations in this thesis were applied according to a predetermined probability. A more effective technique may be to augment every sample, but account for the severity through the modulation of the hyperparameters of the constituent transformations. This was not, however, investigated in this thesis, as the probability-based implementation facilitated more apples-to-apples comparison to Consistency Training. 

\section{Consistency Training and Generalizability}
Consistency training was shown to improve generalizability, outperforming data augmentation by a significant margin on all \gls{ood} datasets. This can be attributed to the additional inductive that are imposed.  
        
    \subsection{Impact}
    Though Consistency Training did increase generalization by a considerable amount, the \gls{ood} performance is nevertheless insufficient for practical purposes. The best performance on Etis-LaribDB with Consistency Training was after all merely 0.504, as shown in~\Cref{tab:consistency}. This kind of performance would naturally be of limited utility in clinical applications.
    
    Consistency Training does, however, constitute a step in the right direction. In contrast to competing methods such as Model-Based Robust Deep Learning \cite{modelbased}, Invariant Risk Minimization \cite{IRM}, or multi-domain training \cite{generalization_datamod}, Consistency Training only requires a single dataset, and can as a result be used in practically every segmentation pipeline. The implementation thereof is also conceptually simple, and can for practical purposes be considered a more generalizable alternative to data augmentation. 
    
    Given further development, Consistency Training may prove a promising candidate as a means of alleviating generalization failure to practically viable extents, especially if leveraged in conjunction with other methods. As established in~\Cref{methods}, the limits are in theory only the efficacy of the quantification of consistency for a given task and the extent to which the augmentation strategy can account for any distributional shifts that may occur. Improvements to either of these aspects are likely to contribute to considerable gains in generalizability. 
    
    Developing perturbation models and consistency metrics may also be a great opportunity to incorporate expert input. A clinician could for instance offer insights as to the nature of the perturbations one might expect in practice and thus assist in the development of the perturbation model.
    
\subsection{Limitations}
    During the experiments performed in this thesis, the batch size was set to 8 for all training procedures. As Consistency Training relies on generating pairs of data from a given batch, one may argue that keeping the batch size the same may result in a weak comparison. The experiment should as such ideally be repeated across a number of batch sizes, but this was infeasible due to logistical constraints, in particular with regards to computational resources. 


\section{Ensembles and Generalizability}
    The use of ensembles, as shown in~\Cref{ensembles}, was proved to increase generalization. The improvements were on average comparatively minor, however increasing generalization by 2.026\%, 3.081\% and 2.351\% respectively for ensembles trained with no augmentation, conventional augmentation, and Consistency Training.  To reiterate, data augmentation contributed an average improvement of 9.00\% and Consistency Training 11.73\% over no augmentation. 
    
    Moreover, the differences in improvements between ensemble training methods was not found to be statistically significant. Thus, the findings in this thesis suggest that ensembles 
    
    
    
    Finally, the findings as presented in~\Cref{fig:ensemble_var} do to some extent support the hypothesis that this improvement is a consequence of the fact that ensembles mitigate underspecification, as the greatest gains to generalization were achieved by models that initially exhibited high degrees of underspecification as quantified by the performance variability of the respective pipelines. This was not, however, proven with statistical certainty, and requires higher sample sizes and ideally a larger diversity of model-architectures to verify to statistically significant extents.  

    
    \subsection{Impact}
    The use of ensembles was in this thesis proven to be a comparatively simple and reliable way of increasing generalization, albeit by a minor amount. It should also be noted that ensembles incur higher costs with regards to training time, time required for inference, and memory requirements. This, naturally, needs to be weighted against the benefits, which as discussed are fairly marginal on average. It may for instance be the case that the computational resources spent training multiple predictors for use in an ensemble would be better spent tuning the augmentation strategy if a \gls{ood} dataset is available. As the results in \Cref{augmentations} show, the choice of augmentation strategy appears to have a more significant impact on generalization than the use of ensembles. Thus, the findings in this thesis suggest that testing N different augmentation strategies may be a better use of resources than training N identical predictors to be used in an ensemble. Granted, this is difficult to say with certainty without exploring a larger diversity of augmentation strategies as discussed in \Cref{augmentation_limitations}.
    
    The analysis of ensembles in the context of underspecification performed in \Cref{ensemble_underspecification} showed that
    
    \subsection{Limitations}
    As mentioned in~\Cref{ensembles}, the constituent predictors for each ensemble were sampled from the ten predictors trained for the purpose of the experiments in~\Cref{augmentations} and~\Cref{consistency_training}. As a result, the statistical significance of the findings are not necessarily robust. Thus, the experiments should ideally be repeated with an increased sample size, for instance N=50, such that ten ensembles could be constructed such that each ensemble consists of an independent set of predictors. 
    
    It should also be noted that the experiments in this thesis were performed only at one ensemble size - i.e, five models. This choice was informed by the literature, in particular the implementation of DivergentNet \cite{divergentnets}. Ensembles may as such have a greater impact than expected, dependent on the returns from increasing the model counts. Following the Bayesian perspective as discussed in~\Cref{background:ensembles}, increasing the model count may result in a better estimate of the Bayesian posterior, and thus lead to increased generalization.
    
    The improvements from increasing ensemble size may however be limited. The performance of ensembles is after all bounded by the performance of perfect Bayesian marginalization. As shown in~\Cref{fig:bayesian_generalization}, this will not necessarily constitute perfect generalization, as the predictions are in such a system weighted according to the likelihood that the given weight configuration is returned from the pipeline. Thus, if learning shortcuts is likely, Bayesian marginalization will primarily be predicting according to shortcut features. Investigating this may be an interesting direction of further study.
    
    Finally, the method by which ensembles were implemented in this thesis - in particular, that the heatmap is thresholded with majority vote - may under-represent the potential utility of ensembles. By requiring that at least half of the constituent predictors are in consensus in order to consider a given pixel as a positive prediction, some potentially insightful predictions may be discarded. A better alternative is to consider the heatmap as a whole, which in any case would be more informative in a clinical setting. Evaluation of ensemble models should thus ideally take this into account. 


\section{Overall Impact}
Though this thesis presents methods that constitute considerable improvements to generalization, the best performing system - namely the DeepLabV3+ ensemble trained with Consistency Training -  would nevertheless not be particularly useful in practical settings when considered holistically.

This system achieved average \glspl{iou} of 0.751 on CVC-ClinicDB, 0.683 on EndoCV2020, 0.523 on Etis-LaribDB, and 0.859 on Kvasir-SEG. Though this constitutes a considerable improvement over both of the "naive" pipelines - i.e single models trained with and without regular data augmentation - it is nonetheless not sufficiently generalizable for practical use. Ideally, there should be negligible differences between all four datasets, and though there is room for some degree of performance degradation, a system that exhibits a mean \gls{iou} of 0.523 is not particularly useful and as discussed in~\Cref{ethics} may actually cause more harm than good. 

Thus, in spite of the aforementioned improvements, the pipeline as a whole is not in purely practical terms much better than any of the naive pipelines. More work is evidently required to achieve suitable levels of generalization. Some ideas for directions of further work towards this end will be discussed in \Cref{conclusion}. 
\section{Limitations of the Experimental Methodology}

\subsection{Metrics selection}

\cite{metric_weakness}
As this thesis focused on the differences in performance between \gls{ood} and \gls{ind} datasets, the only metrics that were considered was the \gls{iou} and the \gls{cs} of the \gls{iou}. Though \gls{iou} is a very popular metric for evaluating segmentation pipelines and is easy to interpret, it is not without its flaws~\cite{metric_weakness}. It is for instance typically biased against small structures, as these are affected to a greater degree by errors that in practical terms are comparatively minor. This can result in misleading mean \glspl{iou}, in particular when the distribution of object sizes is wide, as is indeed often is the case with polyps datasets~\cite{endocv2021_review}.  

Ideally, more metrics should have been considered, for instance precision, recall, and perhaps even \gls{sis}, in order to paint a more complete picture of the performance of the tested methods. 

\subsection{Dataset Selection}
This thesis considered three \gls{ood} datasets throughout all the experiments. Though this provides some indication of generalizability, ideally even more \gls{ood} datasets should have been used. For instance, though Etis-LaribDB was the most difficult of the datasets used in this thesis, the performance on this dataset does not necessarily reflect the worst-case performance in a clinical setting. Indeed, the extent to which the a given pipeline fails to generalize cannot be sufficiently anticipated \cite{trust_ai} given current approaches to deep learning. It may easily be the case that the model performs even worse under certain clinical conditions. Without a larger sample of \gls{ood} datasets, there is a high degree of uncertainty involved as to the actual ability of the given systems to generalize. Though the low generalizability of the systems implemented in this thesis means that this has little practical bearing, any future research that reports generalizability of practical merit should concentrate a significant effort on assembling a large collection of \gls{ood} datasets. 

As briefly mentioned in \Cref{datasets}, the methods presented in this thesis should ideally have been compared to the works presented in EndoCV2021~\cite{endocv2021}. As the datasets used to evaluate the submissions was not available at the time of writing this thesis, however, this was infeasible. 

\subsection{Model Architectures}
To validate that the impact of the proposed methods translated across models, and to determine the impact of model architectures, this thesis implemented five separate models - i.e DeepLabV3+, DD-DeepLabV3+, FPN, Unet, and TriUNet. 

It can be argued that these models do not capture the diversity of segmentation models available, however.

\section{Summary}
This section presented a more thorough analysis and discussion of the findings as presented in~\Cref{experiments}. The results were analyzed with respect to the literature, the theory as laid out in~\Cref{background}, and considered in terms of viability of clinical deployment. For each experiment, the impact, limitations and potential improvements were discussed where applicable. 

Holistically, the findings in this thesis highlight that generalization remains a challenging problem, but that the development of generalizable methods is an endeavor ripe for further exploration. Consistency Training in particular seems to be a promising candidate for further research towards alleviating generalization failure. It was also shown that further foundational work is required in order to fully understand the relationship between the constituent components of the deep learning pipeline and generalizability, in particular with regards to developing sufficiently well-controlled experimental methodologies and eliminating confounding variables during comparative studies. 