    \chapter{Introduction}\label{introduction}

    The last decade or so has seen a veritable revolution in \gls{ai}. This has been spearheaded principally by advancements in Deep Learning, the remarkable performance of which has rendered more conventional approaches practically obsolete. Recent work has, however, highlighted that the models trained using deep learning, i.e., \glspl{dnn}, are highly prone to exhibiting significant reductions in performance when deployed in practical settings despite readily exhibiting high performance when evaluated on previously unseen subsets of the training data~\cite{damour2020underspecification, shortcut_learning, noise_robustness, corruption_robustness}. This is referred to as \textit{generalization failure}. 
    

    This type of behaviour is especially prevalent in the domain of medical imaging. Though medical imaging in recent years has proven to be one of the most promising applications of deep learning, having the capacity to significantly improve both the accuracy and efficiency of the detection, diagnosis, and treatment of a wide variety of diseases~\cite{dl_medical_imaging}, recent research has shown that these types of systems are particularly susceptible to generalization failure. Whereas other domains often have access to exceedingly large datasets, medical datasets are typically fairly small both due to privacy concerns and the high costs associated with annotation. Moreover, even when large datasets are available, they are unlikely to fully encapsulate the nature of whatever relationships they are intended to represent due to the inherent variability present in medical domains. There are often high degrees of variability within the same class of pathology, which in addition to multitudes of confounding variables, such as differences in clinical routines, demographics, imaging equipment, and so on typically result in \glspl{dnn} exhibiting high degrees of sensitivity to even minor changes in the nature of the input data. Finally, since medical datasets are typically collected from a single hospital, from a limited demographic, and with a limited selection of equipment, sampling bias is practically unavoidable~\cite{bias, bias2}. In addition to the fact that this reduces the overall clinical utility of the system, it may also induce certain societal consequences if deployed~\cite{social_consequence_1}. 

    This all exacerbates the already difficult task of making deep learning systems sufficiently generalizable for practical use. Indeed, even \glspl{dnn} trained on enormous non-medical datasets exhibit high degrees of sensitivity to distributional shifts, for example due to changes in texture~\cite{texturebias}, additive noise~\cite{noise_robustness}, and minor image corruptions~\cite{corruption_robustness}. Even perturbations imperceptible to the human eye in the form of \textit{adversarial attacks} can break even the most sophisticated deep learning systems~\cite{adversarial_attacks}.
    
    Thus, ensuring that the performance of deep-learning-based systems is generalizable and sufficiently robust is a matter of particular importance in medical domains. Whereas more general deep learning pipelines can sometimes avoid generalization failure by virtue of the sheer volume of training data available, the constraints imposed by the medical domain necessitate more carefully designed pipelines, with a particular focus on ensuring maximal generalizability. 
    
    Conventional implementations of deep-learning systems tend to neglect this fact, with high performance on unseen subsets of the training dataset typically being considered a sufficient indicator of generalization. This is highly misleading as to the actual performance of the given model should it be deployed in a setting where the nature of the data may differ, however, even if the differences between the two domains are slight or even unremarkable to a human observer. Such data is often referred to as being \textit{\gls{ood}}. There has been limited research addressing the development of methods that facilitate generalization to such data, in large part because the mechanisms behind generalization failure are currently only in the beginning stages of being understood to an actionable degree.
    
    The EndoCV2021 competition~\cite{endocv2021} was organized in order to motivate such research in the context of detection- and segmentation of colorectal polyps. This body of work and the advancements it brought, along with the multitude of different datasets available in this domain, means that polyp segmentation constitutes a compelling candidate for a case-study towards understanding generalization failure and developing generalizable methods. The polyp segmentation task will as a consequence serve as the primary context of the work presented in thesis.
    
    \section{Case study: Colon Polyp Segmentation}   
    Colorectal cancer is one of the leading causes of cancer related deaths, causing approximately 900 thousand deaths worldwide per year~\cite{colorectal_cancer}. Early detection and subsequent resection of polyps, a precursor to colorectal cancer, is as such of significant importance towards reducing the incidence- and mortality-rates thereof. Polyps are, however, easily missed during colonoscopies due to the significant variability in their shapes and sizes, as well as the high degree of visual similarity to surrounding tissue~\cite{missrate1, missrate2}. 
    
    Automatic segmentation of polyps via deep learning has as a consequence been identified as a promising candidate for reducing polyp miss-rates by serving as an auxiliary detection method during screening. There has been a wealth of work dedicated to developing such systems~\cite{endotect, endocv2020, endocv2021, ddanet}, with some studies reporting that AI-assisted detection may increase detection rates by 10\% in clinical deployment~\cite{polyp-success-story}.  
    
    As mentioned, however, these sorts of systems have also been shown to be highly prone to generalization failure~\cite{damour2020underspecification, shortcut_learning, endocv2021_review}. To address this, the EndoCV2021 competition~\cite{endocv2021} was organized with the primary goal of developing generalizable deep learning systems for polyp-segmentation and -detection. The submissions were evaluated on several unseen datasets consisting of endoscopic images collected from a separate center than the provided training datasets as well as images collected using a differing endoscopic lighting system. Though several teams made good progress towards increasing generalizability, the organizers' review of the submissions~\cite{endocv2021_review} highlighted that every submitted model nevertheless exhibited significant performance reductions on the aforementioned unseen datasets. 
    % Moreover, though a multitude of methods and approaches were tested, many of which did indeed benefit generalizability, few methods stood out as having potential for significant further development.

    Furthermore, there has at the time of writing this thesis been limited research dedicated to developing an understanding of the relative impact of the many design elements present in most deep learning pipelines on generalization. Indeed, most analyses performed today, therein those performed in the EndoCV2021 review\cite{endocv2021_review}, do not explicitly control potentially highly affecting variables - such as the choice of data augmentation strategies -  when comparing methods, instead only considering the performance of the final system. There is as a consequence a somewhat limited understanding of the relative impacts of the many methods and techniques believed to improve generalization.  
    
    \section{Research Objectives}
    This thesis aims to build upon and synthesize the findings reported in EndoCV2021 and other recent works on generalizability. The overall goal of this thesis is as such to explore methods of increasing the generalizability of deep-learning-based polyp-segmentation systems. This goal can be decoupled into the following pair of research objectives:
    \begin{enumerate}
        \item \textbf{To leverage recent advances in the understanding of generalization failure to inform the development of novel methods of increasing the generalization of deep learning systems for polyp-segmentation}. By synthesizing the often fragmented analyses of generalization failure presented in the literature, one can develop a more holistic understanding of why these failures occur and the mechanisms behind them. This, naturally, facilitates the development of more targeted methods towards increasing generalizability. \label{cont_1}

        \item \textbf{To synthesize recent work on generalizability and determine concretely the degree to which more conventional and well-established methods affect generalization}. Deep Learning systems are highly complex, with several moving parts and complicated dynamics. Analyzing the impact of the constituent components thereof on generalization is as such warranted. In particular, this thesis compares the impact of the following variables: the choice of model architecture, the use of data augmentation, and the use of ensembles. Though this constitutes a somewhat limited selection of the many methods and components that may make up a modern deep learning system, it is often these that are subject to the most research. Among the eight submissions to the segmentation portion of EndoCV2021, for instance, three primarily made use of ensembles~\cite{endocv2021_ensemble_3, divergentnets, endoensemble}, two developed novel model-architectures~\cite{endocv2021_gru, doubleencdec}, and one developed an augmentation strategy~\cite{polyp_augmentation}.  \label{cont_2}
    \end{enumerate}
   
    \section{Main Contributions and Key Findings}
    Objective \ref{cont_1} was achieved through the development of the following novel methods:
    \begin{itemize}
        \item A framework for analyzing generalizability based on reframing it as the model's ability to output predictions that are consistent across distributional shifts.
        \item A metric and loss function intended to quantify this notion of consistency in the context of segmentation, referred \glsfirst{sis} and \glsfirst{sil} respectively.
        \item A custom augmentation strategy intended to induce the aforementioned distributional shifts in a controlled manner, leveraging both conventional augmentations and a \glsfirst{gan} which generates synthetic polyps in a given image. 
        \item A training paradigm which makes use of the aforementioned framework, loss function and augmentation strategy, referred to as Consistency Training. In contrast to many competing methods, this does not require multiple datasets, and is for practical purposes a more generalizable alternative to data augmentation. This method was also the basis for a research paper submitted to NeurIPS 2022, which can be found in \Cref{Paper}.
        \item Several ensembles consisting of models trained according to the the aforementioned training paradigm.
        \item A simple dual-decoder model, wherein one decoder performs image reconstruction and the other segmentation. This is intended to facilitate the learning of more generalizable features.
    \end{itemize}
    
    These methods were then evaluated through four separate experiments. To fully understand the relative impacts of these methods, several baselines were also implemented, varying the model architecture, augmentation strategy, and the use of ensembles. These baselines were then compared both to the novel methods as presented above, and to one another in order to ascertain the individual impacts, hence achieving Objective \ref{cont_2}.
    
    The results from these experiments were then analyzed with respect to theoretical frameworks presented in~\Cref{background}. The findings from these analyses then informed a number of hypotheses which were suggested as points of further study. The most notable findings can be summarized as follows:
    \begin{itemize}
        \item Consistency training greatly increased generalizability, outperforming every other tested method. Several possible improvements to Consistency Training were also presented, along with ideas for more advanced training methods that make use of the Consistency framework.
        
        \item Data augmentation also increased generalizability, albeit by a somewhat smaller margin than Consistency Training. When the augmentation strategy incorporated a generative inpainter, the gains were marginally less substantial. It was argued that the extent to which the use of data augmentation affects generalization raises questions as to the veracity of comparative studies that do not account for the use of disparate augmentation strategies, therein EndoCV2021.

        \item Ensemble models improve generalization by a minor amount when compared to the mean performance of the models that make them up. Similar improvements could be observed regardless of the training procedure and model architecture used, suggesting perhaps unsurprisingly that the generalizability of ensembles is primarily determined by the generalizability of the constituent models. It was also shown that the gains from ensembles is correlated with the variability in performance between the constituent models. A diversity-based training method for ensemble models was suggested to investigate this further.
        
        \item The model architectures tested in this thesis all exhibited fairly comparable degrees of generalizability. The introduced dual-decoder model did not contribute to increased generalization. After analyzing this result, it was theorized that segmentation encoders readily learn task-invariant features and primarily simply perform image compression. Further experiments were suggested to investigate this.  
    \end{itemize}

    Though this work by no means solves the problem of generalization failure, the aforementioned contributions constitute a significant step in the right direction. Consistency Training in particular appears to be a highly promising concept, with plenty of room for further development. 
    
    \section{Research Methods}
    The research methods used in this thesis were principally of an exploratory and quantitative nature~\cite{research_methods}. The methods were analyzed quantitatively, and the relative performance of each method determined to statistical significance where applicable. The analysis of these findings and the resulting theories explaining them with respect to the theory was, however, exploratory, as was the development process for the novel methods. This approach was chosen due to the inherently dualistic nature of the thesis as per the research objectives; a quantitative approach permits statistically significant comparisons between methods, whereas an exploratory approach affords flexibility with regards to the development of the methods as well as permitting sufficient discussion of the quantitative findings with respect to the established theoretical frameworks. 
    
    
    \section{Organization of the Thesis}
    The thesis will be organized as follows:
    \begin{itemize}
        \item ~\Cref{background} will cover all relevant background knowledge. This includes a brief introduction to polyps and their role in colorectal cancer, deep learning, and segmentation, as well as an overview and synthesis of related works on generalizability failure and generalizable methods for deep learning. Ethical considerations pertaining to generalizability will also be discussed.
        \item ~\Cref{methods} will cover the methods that constitute the contributions as outlined above, their basis with respect to the theory presented in \Cref{background}, as well as details surrounding their implementation where applicable.
        \item ~\Cref{experiments} will describe the experimental setup, present the experiments, and present the results thereof and discuss them in the context of the established theory.
        \item ~\Cref{discussion} will discuss these findings in further detail, as well as the impacts and limitations thereof.
        \item Finally,~\Cref{conclusion} summarizes the work done in this thesis along with presenting directions for further research.
    \end{itemize} 