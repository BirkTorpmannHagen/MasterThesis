    \chapter{Introduction}\label{introduction}

    The last decade or so has seen a veritable revolution in \gls{ai}. This has been spearheaded principally by advancements in Deep Learning, the remarkable performance of which has rendered more conventional approaches practically obsolete. Recent work has however highlighted that the models trained using deep learning - i.e \glspl{dnn} are highly prone to exhibiting significant reductions in performance when deployed in practical settings, in spite of the fact that they readily exhibit high performance when evaluated on previously unseen subsets of the training data \cite{damour2020underspecification, shortcut_learning, noise_robustness, corruption_robustness}. This is referred to as \textit{generalization failure}. 
    
    This type of behaviour is especially prevalent in the domain of medical imaging. Though medical imaging in recent years has proven to be one of the most promising applications of deep learning, having the capacity to significantly improve both the accuracy and efficiency of the detection, diagnosis, and treatment of a wide variety of diseases \cite{dl_medical_imaging}, recent research has shown that these types of systems are particularly susceptible to generalization failure. Whereas other domains often have access to exceedingly large datasets, medical datasets are typically fairly small due to privacy concerns and high costs associated with annotation. Moreover, even when large datasets are available, they are unlikely to fully encapsulate the nature of whatever relationships they are intended to represent due to the inherent variability present in medical domains. There are often high degrees of variability within the same class of pathology, which in addition to multitudes of confounding variables, such as differences in clinical routines, demographics, imaging equipment and so on typically result in \glspl{dnn} exhibiting high degrees of sensitivity to even minor changes in the inputs. Finally, since the data is typically collected from a single hospital, from a limited demographic, and with a limited selection of equipment, sampling bias is practically unavoidable \cite{bias, bias2}. In addition to the fact that this reduces the clinical utility of the system, it may also induce certain societal consequences if deployed \cite{social_consequence_1}. 
    
    This all exacerbates the already difficult task of making deep learning systems sufficiently generalizable for practical use. Indeed, even \glspl{dnn} trained on enormous non-medical datasets exhibit high degrees of sensitivity to distributional shifts, for example due to changes in texture \cite{texturebias}, additive noise \cite{noise_robustness}, and minor image corruptions \cite{corruption_robustness}. Even perturbations imperceptible to the human eye in the form of \textit{adversarial attacks} can break even the most sophisticated deep learning systems \cite{adversarial_attacks}.
    
    Thus, ensuring that the performance of such systems is generalizable and sufficciently robust is a matter of particular importance when developing deep learning systems for medical domains. Whereas more general deep learning pipelines can sometimes avoid the problem of generalizability by virtue of the sheer volume of training data available, the constraints imposed by the medical domain necessitate more carefully designed pipelines, with a particular focus on ensuring maximal generalizability. 
    
    Conventional implementations of deep-learning based systems tend to neglect this fact, however. Instead, high performance on a hold-out set is typically considered a sufficient indicator of generalization. This, as will be discussed in \Cref{background}, is highly misleading as to the actual performance of the given model should it be deployed in a practical setting. The field is still in the early stages of fully understanding why such generalization failure occurs, and there has consequently until recently been limited research explicitly targeting the development of generalizable methods. 
    
    An attempt was made to address these shortcomings in the context of detection- and segmentation of colorectal polyps via the EndoCV2021 competition \cite{endocv2021}, however. This body of work, along with the multitude of different datasets available in this domain means that polyp segmentation may serve as a compelling candidate for a case-study towards understanding generalization failure and developing generalizable methods, and will as such be the focus of this thesis.   
    
    \section{EndoCV2021 and Colon Polyp Image Segmentation}   
    Colorectal cancer is one of the leading causes of cancer related deaths, causing approximately 900 thousand deaths worldwide per year \cite{colorectal_cancer}. Early detection and subsequent resection of polyps, a precursor to colorectal cancer, is as such of significant importance towards reducing the incidence- and mortality-rates thereof. Polyps are, however, easily missed during colonoscopies due to the significant variability in the shapes and sizes of polyps, as well as the high degrees of similarity to surrounding tissue \cite{missrate1, missrate2}. 
    Automatic segmentation of polyps via deep learning has as a consequence been identified as a promising candidate for reducing polyp miss-rates by serving as an auxiliary detection method during screening. There has been a wealth of work dedicated to developing such systems, with some studies reporting that AI-assisted detection may increase detection rates by 10\% in clinical deployment ~\cite{polyp-success-story}.  
    
    As mentioned, however, these systems have also been shown to be highly prone to generalization failure. To address this, the EndoCV2021 competition was organized with the primary goal of developing generalizable deep learning systems for polyp-segmentation and -detection. The submissions were evaluated on unseen datasets, consisting of endoscopic images collected from entirely separate centers than the provided training datasets. Though several teams made good progress towards increasing generalizability, the proceedings highlighted that every submitted model nevertheless exhibited significant performance reductions on these unseen datasets. Moreover, though a multitude of methods and approaches were tested, many of which did indeed benefit generalizability, few of methods stood out as having the potential for significant further development.

    Finally, there has at the time of writing this thesis been limited research dedicated to developing an understanding of the relative impact of the many design elements present in most deep learning pipelines on generalization. Indeed, most analyses performed today, therein those performed in EndoCV2021, do not control potentially highly affecting variables - such as the choice of data augmentation strategies -  when comparing methods, instead only considering the performance of the final system. What this means, in effect, is that there is only a very limited understanding of the relative impacts of the many methods and techniques believed to improve generalization.  
    
    \section{Research Objectives}
    This thesis aims to build upon and synthesize the findings reported in EndoCV2021 and other recent works on generalizability. The research objectives considered throughout this thesis are twofold:
    \begin{enumerate}
        \item To develop novel methods of increasing the generalization of deep learning models for polyp-segmentation \label{cont_1}
        \item To synthesize recent work on generalizability and determine concretely the degree to which more conventional and well-established methods affect generalization. In particular, this thesis compares the impact of the following variables: the choice of model architecture, the use of data augmentation, and the use of ensembles. \label{cont_2}
    \end{enumerate}
   
    \section{Main Contributions}
    Objective \ref{cont_1} was achieved through the development of the following novel methods:
    \begin{itemize}
        \item A novel framework for quantifying and thinking about generalization as the consistency of a given model's predictions when its inputs are subjected to a distributional shift.
        \item A metric and loss function intended to quantify this notion of consistency in the context of segmentation.
        \item A custom augmentation strategy, leveraging both conventional augmentations and a \glsfirst{gan}.
        \item A training paradigm which makes use of aforementioned framework, loss function and augmentation strategy, referred to as consistency training.
        \item Several ensembles consisting of models trained according to the the aforementioned training paradigm.
        \item A simple dual-decoder model, wherein one decoder performs image reconstruction and the other segmentaton. This is intended to facilitate the learning of more generalizable features.
    \end{itemize}
    
    These methods were then evaluated through a quantitative comparative study. To fully understand the relative impacts of these methods, several baselines were also implemented, varying the model architecture, augmentation strategy, and the use of ensembles. These baselines were then compared both to the novel methods as presented above, and to one another in order to ascertain the individual impacts, hence achieving Objective \ref{cont_2}.
    
    Four experiments were conducted. For each experiment, ten samples were collected for each of the configurations being tested. The first experiment considered the impact of model architectures, including the double-decoder model. Ten instances of each model were as such trained without augmentation. The next experiment investigated the impact of data augmentation, including the generative inpainter. Ten instances of each model were then trained for each augmentation method - either no augmentation, conventional augmentation, or conventional augmentation with the inpainter. The following experiment then investigated the impact of consistency training, and compared this to the impact of data augmentation, which consistency training in effect can be considered an alternative to. Finally, the impact of ensembles consisting of models trained with consistency training was determined, and compared to the non-ensemble equivalents. 
    
    The results from these experiments were then analyzed with respect to theoretical frameworks presented in \Cref{background}. The findings from these analyses then informed a number of hypotheses which were suggested as points of further study. The key findings can be summarized as follows:
    \begin{itemize}
        \item The dual-decoder model did not contribute to increased generalization. It was argued that this may be due to the type of features the encoder had learned.
    
        \item Consistency training greatly increases generalizability, outperforming every other tested method. Several possible improvements of consistency training were also presented, along with ideas for more advanced training frameworks.
        
        \item Data augmentation exh a significant impact, but performed worse than Consistency Training. It was argued that this fact raises questions as to the veracity of comparative studies that do not account for disparate data augmentation strategies, therein EndoCV2021.
    
        \item Ensemble models improved generalization, but by margins that appeared to be dependent on the performance variability of the constituent model(s). A diversity-based training method for ensemble models was suggested as a direction of further study to investigate this.
    \end{itemize}

    Though this work by no means solves the problem of generalization failure, the aforementioned contributions constitute a significant step in the right direction. Consistency Training in particular appears to be a highly promising concept, with plenty of room for further development. 
    
    \section{Organization of the thesis}
    The thesis will be organized as follows: \Cref{background} will cover all relevant background knowledge. This includes a brief introduction to polyps and their role in colorectal cancer, deep learning, and segmentation, as well as an overview and synthesis of related works on generalizability failure and generalizable methods for deep learning. \Cref{methods} will then cover the methods that constitute the contributions as outlined above, their basis with respect to the presented theory, and their impleentation. \Cref{experiments} will then describe the experimental methodology, present the experiments and results, and briefly discuss the results from the individual experiments. \Cref{discussion} will then analyze these findings in further detail, as well as the impacts and limitations thereof, along with presenting directions for further research. Finally, \Cref{conclusion} summarizes the work done in this thesis.