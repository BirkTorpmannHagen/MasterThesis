    \chapter{Introduction}
    %research goals
    %research motivation
    %thesis outline
    Colorectal cancer is one of the leading causes of cancer related deaths, causing approximately 900 thousand deaths worldwide per year \cite{colorectal_cancer}. Early detection and consequent resection of polyps, a precursor to colorectal cancer, is as such of significant importance towards reducing incidence- and mortality-rates thereof. Polyps are, however, often missed during colonoscopies, owing to the significant variability in the shapes and sizes of polyps, as well as the high degrees of similarity to surrounding tissue \cite{missrate1, missrate2}. 
    
    Automatic segmentation of polyps via deep learning has as a consequence been identified as a promising candidate for reducing polyp miss-rates by serving as an auxiliary detection method during screening. There has as a result been a wealth of work dedicated to developing such systems, with some studies reporting that AI-assisted detection can increase detection rates by significant margins \cite{polyp-success-story}.  
    
    %generalizability
    Recent work has, however, highlighted that deep neural networks \glspl{dnn} readily fail to maintain sufficient performance when deployed outside of lab-conditions \cite{damour2020underspecification, shortcut_learning, endocv2021}. This is known as generalization failure, and has been shown to be ubiquitous across practically every application of deep learning. Understanding exactly how and why this is the case and understanding how is a subject of ongoing study, and there has as a result until recently been limited work explicitly targeting the development of methods that reduce the generalizability gap. To address this, the EndoCV2021 challange was organized, wherein deep-learning based polyp segmentation and detection models were evaluated on unseen, \gls{ood} datasets, consisting of polyp-images collected from a separate center than the training sets. Though several teams made good progress towards increasing generalizability, the proceedings still highlighted that every submitted model exhibited significant performance reductions on the unseen dataset.

    %My methods
    This thesis aims to build upon the EndoCV2021 findings and related works that have been published in the field of generalizable deep learning in the last few years. Generalizability theory is considered from first principles, and the various studies on the matter synthesized in order to create a framework for understanding and addressing the various issues that contribute to generalization failure. This is in turn used to inform the development of a novel pipeline dedicated especially to increasing generalizability, wherein the contributions are as follows:
    \begin{itemize}
        \item A novel approach to quantifying and thinking about generalization as the consistency of a given model's predictions when the inputs are subjected to perturbations 
        \item A new metric and loss function intended to quantify this notion of consistency in the context of segmentation
        \item A custom augmentation strategy, leveraging both conventional augmentations and a \gls{gan}.
        \item A training paradigm which makes use of aforementioned loss function and augmentation strategy
        \item Several forms of ensembles consisting of models trained according to the the aforementioned training paradigm
        \item A simple double-decoder model, wherein one decoder performs image reconstruction, intended to facilitate the learning of more generalizable features
    \end{itemize}
    
    This pipeline was then evaluated through a multitude of experiments,
    

    To sufficiently evaluate the impact of the respective components, an ablation study was conducted - i.e, each of the components were tested individually, and in conjunction with one another should there be some dynamic between some number of them conducive to increased generalization. This way, the thesis also contributes a rigorous analysis of the impact of the types of methods used to increase generalizability in the literature, which at the time of writing this thesis has not yet been performed. Though this analysis is by no means complete, it provides a general overview of the techniques used in the literature, their impact on generalization, and theoretical analyses of why they perform the way they do.
    
    The results indicate that data-augmentation increased generalizabiltiy more than any other method tested in this hesis. The impact of the new loss-function and validation procedure was shown to have statistically insignificant impact, and can be mathematically proven to be equivalent to conventional data augmentation up to choice of hyperparameters. The new metrics were, however, highly correlated with \gls{ood} performance, and may nevertheless be helpful for benchmarking generalizability when \gls{ood} data is unavailable. Finally, ensembles were also shown to have a significant positive impact, though this was largely dependent on (...)  
    %Analysis
    (Analysis)

    % Organization
    The thesis will be organized as follows: the next chapter will cover all relevant background knowledge. This includes a brief introduction to polyps and their role in colorectal cancer, deep learning, and segmentation, and then an overview and synthesis of related works on generalizability and generalizable methods for deep learning. The subsequent chapter will then cover the contributions as outlined above, their basis with respect to the presented theory, and finally how they were implemented. Chapter \ref{experiments} will then describe the experimental procedures and briefly discuss the results from the individual experiments. Chapter \ref{analysis} will then analyze these findings in further detail, including some surface-level mathematical analysis. Finally, the implications of these findings will be discussed in chapter \ref{discussion} along with plans and suggestions for further work on the topic. 