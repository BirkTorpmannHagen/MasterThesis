    \chapter{Introduction}\label{introduction}

    The last decade or so has seen a veritable revolution in \gls{ai}. This has been spearheaded principally by advancements in Deep Learning, the remarkable performance of which has rendered more conventional approaches practically obsolete. Recent work has, however, highlighted that the models trained using deep learning, i.e., \glspl{dnn}, are highly prone to exhibiting significant reductions in performance when deployed in practical settings despite readily exhibiting high performance when evaluated on previously unseen subsets of the training data~\cite{damour2020underspecification, shortcut_learning, noise_robustness, corruption_robustness}. This is referred to as \textit{generalization failure}. 
    

    This type of behaviour is especially prevalent in the domain of medical imaging. Though medical imaging in recent years has proven to be one of the most promising applications of deep learning, having the capacity to significantly improve both the accuracy and efficiency of the detection, diagnosis, and treatment of a wide variety of diseases~\cite{dl_medical_imaging}, recent research has shown that these types of systems are particularly susceptible to generalization failure. Whereas other domains often have access to exceedingly large datasets, medical datasets are typically fairly small both due to privacy concerns and the high costs associated with annotation. Moreover, even when large datasets are available, they are unlikely to fully encapsulate the nature of whatever relationships they are intended to represent due to the inherent variability present in medical domains. There are often high degrees of variability within the same class of pathology, which in addition to multitudes of confounding variables, such as differences in clinical routines, demographics, imaging equipment and so on typically result in \glspl{dnn} exhibiting high degrees of sensitivity to even minor changes to the inputs. Finally, since medical datasets are typically collected from a single hospital, from a limited demographic, and with a limited selection of equipment, sampling bias is practically unavoidable~\cite{bias, bias2}. In addition to the fact that this reduces the clinical utility of the system, it may also induce certain societal consequences if deployed~\cite{social_consequence_1}. 
    

    This all exacerbates the already difficult task of making deep learning systems sufficiently generalizable for practical use. Indeed, even \glspl{dnn} trained on enormous non-medical datasets exhibit high degrees of sensitivity to distributional shifts, for example due to changes in texture~\cite{texturebias}, additive noise~\cite{noise_robustness}, and minor image corruptions~\cite{corruption_robustness}. Even perturbations imperceptible to the human eye in the form of \textit{adversarial attacks} can break even the most sophisticated deep learning systems~\cite{adversarial_attacks}.
    
    Thus, ensuring that the performance of such systems is generalizable and sufficiently robust is a matter of particular importance when developing deep learning systems for medical domains. Whereas more general deep learning pipelines can sometimes avoid the problem of generalizability by virtue of the sheer volume of training data available, the constraints imposed by the medical domain necessitate more carefully designed pipelines, with a particular focus on ensuring maximal generalizability. 
    
    Conventional implementations of deep-learning based systems tend to neglect this fact, however. Instead, high performance on hold-out sets is typically considered a sufficient indicator of generalization. This is highly misleading as to the actual performance of the given model should it be deployed in a practical setting. The field is still in the early stages of fully understanding why such generalization failure occurs, and there has consequently until recently been limited research explicitly targeting the development of generalizable methods. 
    
    An attempt was made to address these shortcomings in the context of detection- and segmentation of colorectal polyps via the EndoCV2021 competition~\cite{endocv2021}. This body of work, along with the multitude of different datasets available in this domain means that polyp segmentation may serve as a compelling candidate for a case-study towards understanding generalization failure and developing generalizable methods, and will as such be the focus of this thesis.   
    
    \section{Case study: Colon Polyp Segmentation}   
    Colorectal cancer is one of the leading causes of cancer related deaths, causing approximately 900 thousand deaths worldwide per year~\cite{colorectal_cancer}. Early detection and subsequent resection of polyps, a precursor to colorectal cancer, is as such of significant importance towards reducing the incidence- and mortality-rates thereof. Polyps are, however, easily missed during colonoscopies due to the significant variability in their shapes and sizes, as well as their high degrees of visual similarity to surrounding tissue~\cite{missrate1, missrate2}. 
    
    Automatic segmentation of polyps via deep learning has as a consequence been identified as a promising candidate for reducing polyp miss-rates by serving as an auxiliary detection method during screening. There has been a wealth of work dedicated to developing such systems \cite{endocv2020, endocv2021, ddanet}, with some studies reporting that AI-assisted detection may increase detection rates by 10\% in clinical deployment~\cite{polyp-success-story}.  
    
    As mentioned, however, these systems have also been shown to be highly prone to generalization failure~\cite{endocv2021}. To address this, the EndoCV2021 competition \cite{endocv2021} was organized with the primary goal of developing generalizable deep learning systems for polyp-segmentation and -detection. The submissions were evaluated on unseen datasets, consisting of endoscopic images collected from entirely separate centers than the provided training datasets. Though several teams made good progress towards increasing generalizability, the proceedings highlighted that every submitted model nevertheless exhibited significant performance reductions on these unseen datasets. Moreover, though a multitude of methods and approaches were tested, many of which did indeed benefit generalizability, few methods stood out as having the potential for significant further development.

    Finally, there has at the time of writing this thesis been limited research dedicated to developing an understanding of the relative impact of the many design elements present in most deep learning pipelines on generalization. Indeed, most analyses performed today, therein those performed in EndoCV2021, do not control potentially highly affecting variables - such as the choice of data augmentation strategies -  when comparing methods, instead only considering the performance of the final system. There is as a consequence a somewhat limited understanding of the relative impacts of the many methods and techniques believed to improve generalization.  
    
    \section{Research Objectives}
    This thesis aims to build upon and synthesize the findings reported in EndoCV2021 and other recent works on generalizability. The overall goal of this thesis is as such to explore methods of increasing the generalizability of deep-learning-based polyp-segmentation systems. This goal can be decoupled into the following pair of research objectives:
    \begin{enumerate}
        \item \textbf{To leverage recent advances in the understanding of generalization failure to inform the development of novel methods of increasing the generalization of deep learning systems for polyp-segmentation}. By synthesizing the often fragmented analyses of generalization failure presented in the literature, one can develop a more holistic understanding of why these failures occur and the mechanisms behind them. This, naturally, facilitates the development of more targeted methods towards increasing generalizability. \label{cont_1}

        \item \textbf{To synthesize recent work on generalizability and determine concretely the degree to which more conventional and well-established methods affect generalization}. Deep Learning systems are highly complex, with several moving parts and complicated dynamics. Analyzing the impact of the constituent components thereof on generalization is as such warranted. In particular, this thesis compares the impact of the following variables: the choice of model architecture, the use of data augmentation, and the use of ensembles. \label{cont_2}
    \end{enumerate}
   
    \section{Main Contributions and Key Findings}
    Objective \ref{cont_1} was achieved through the development of the following novel methods:
    \begin{itemize}
        \item A novel framework for quantifying and thinking about generalization as the consistency of a given model when its inputs are subjected to a distributional shift.
        \item A metric and loss function intended to quantify this notion of consistency in the context of segmentation.
        \item A custom augmentation strategy, leveraging both conventional augmentations and a \glsfirst{gan}.
        \item A training paradigm which makes use of the aforementioned framework, loss function and augmentation strategy, referred to as Consistency Training. In contrast to many competing methods, this does not require multiple datasets, and is in effect a more generalizable alternative to data augmentation. This method was also the basis for a research paper submitted to NeurIPS 2022, which can be found in Appendix D.
        \item Several ensembles consisting of models trained according to the the aforementioned training paradigm.
        \item A simple dual-decoder model, wherein one decoder performs image reconstruction and the other segmentation. This is intended to facilitate the learning of more generalizable features.
    \end{itemize}
    
    These methods were then evaluated through four separate experiments. To fully understand the relative impacts of these methods, several baselines were also implemented, varying the model architecture, augmentation strategy, and the use of ensembles. These baselines were then compared both to the novel methods as presented above, and to one another in order to ascertain the individual impacts, hence achieving Objective \ref{cont_2}.
    
    The results from these experiments were then analyzed with respect to theoretical frameworks presented in~\Cref{background}. The findings from these analyses then informed a number of hypotheses which were suggested as points of further study. The key findings can be summarized as follows:
    \begin{itemize}
        \item The dual-decoder model did not contribute to increased generalization. It was argued that this may be due to the type of features the encoder had learned.
    
        \item Consistency training greatly increases generalizability, outperforming every other tested method. Several possible improvements of consistency training were also presented, along with ideas for more advanced training frameworks.
        
        \item Data augmentation increases generalizability by a significant impact, albeit by lesser margins than Consistency Training. It was argued that this fact raises questions as to the veracity of comparative studies that do not account for disparate data augmentation strategies, therein EndoCV2021.
    

        \item Ensemble models improved generalization. The extent thereof appears to be determined by the performance variability of the constituent model(s), but this could not be proven conclusively. A diversity-based training method for ensemble models was suggested to investigate this further.
    \end{itemize}

    Though this work by no means solves the problem of generalization failure, the aforementioned contributions constitute a significant step in the right direction. Consistency Training in particular appears to be a highly promising concept, with plenty of room for further development. 
    
    \section{Research Methods}
    The research methods used in this thesis were principally of an exploratory and quantitative nature \cite{research_methods}. The methods were analyzed quantitatively, and the relative performance of each method determined to statistical significance where applicable. The analysis of these findings and the resulting theories explaining them with respect to the theory was, however, exploratory, as was the development process for the novel methods. This approach was chosen due to the inherently dualistic nature of the thesis as per the research objectives; a quantitative approach permits statistically significant comparisons between methods, whereas an exploratory approach affords flexibility with regards to the development of the methods as well as permitting sufficient discussion of the quantitative findings with respect to the established theoretical frameworks. 
    
    
    \section{Organization of the Thesis}
    The thesis will be organized as follows:
    \begin{itemize}
        \item ~\Cref{background} will cover all relevant background knowledge. This includes a brief introduction to polyps and their role in colorectal cancer, deep learning, and segmentation, as well as an overview and synthesis of related works on generalizability failure and generalizable methods for deep learning.
        \item ~\Cref{methods} will cover the methods that constitute the contributions as outlined above, their basis with respect to the presented theory, and their implementation.
        \item ~\Cref{experiments} will describe the experimental methodology, present the experiments and results, and briefly discuss the results from the individual experiments.
        \item ~\Cref{discussion} will analyze these findings in further detail, including the impacts and limitations thereof as well as suggesting potential improvements and directions of further study for the respective methods.
        \item Finally,~\Cref{conclusion} summarizes the work done in this thesis along with presenting directions for further research.
    \end{itemize} 